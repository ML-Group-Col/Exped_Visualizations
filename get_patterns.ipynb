{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.6 64-bit ('base': conda)",
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "f515a261c91f2d540a32fd338636eea76c71c483334d8e72b3eaaa34365c9198"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Pbc4cip- Python Implementation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import weka.core.jvm as jvm\n",
    "import weka.core.packages as packages\n",
    "from weka.core.classes import complete_classname\n",
    "from weka.core.converters import Loader\n",
    "from weka.classifiers import Classifier\n",
    "import weka.plot.graph as graph  # NB: pygraphviz and PIL are required\n",
    "from weka.core.classes import Random, from_commandline\n",
    "import weka.core.serialization as serialization\n",
    "from weka.filters import Filter\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "source": [
    "## Criteria"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Michael\n",
    "#wekafiles_path = \"C:/Users/mzenk/wekafiles\"\n",
    "#PBC4CIP_zip_path = \"C:/Users/mzenk/Google Drive/ITESM/Maestría/Semestre 3/ML2/Assignment4/PBC4cip-1.0-weka.zip\"\n",
    "#data_dir = \"C:/Users/mzenk/Google Drive/ITESM/Maestría/Semestre 3/ML2/Assignment4/Exped_Visualizations/\"\n",
    "#arff_file = \"weka_dataset_clean_noop_nocorreo.arff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dany\n",
    "wekafiles_path = \"/Users/dannygc/wekafiles\"\n",
    "PBC4CIP_zip_path = \"/Users/dannygc/Google Drive File Stream/My Drive/MCCNotes/MCC 3/Adv_ML/PBC_HW4/PBC4cip-1.0-weka.zip\"\n",
    "data_dir = \"/Volumes/GoogleDrive/My Drive/MCCNotes/Jlab projects/GITHUB_repositories/DRAE_repositories/Exped_Visualizations/\"\n",
    "arff_file = \"weka_dataset_clean_.arff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pd.read_csv('universities.csv',  encoding ='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir['cit2017_>=80000'] = data_dir.cit2017[data_dir.cit2017 >= 80000]\n",
    "data_dir['cit2017_<80000'] = data_dir.cit2017[data_dir.cit2017 < 80000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir['citPA2017_>10'] = data_dir.citPA2017[data_dir.citPA2017 > 10]\n",
    "data_dir['citPA2017_>10'] = data_dir.citPA2017[data_dir.citPA2017 > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir['citPA2017_>10'] = data_dir.citPA2017[data_dir.citPA2017 > 10]\n",
    "data_dir['citPA2017_>10'] = data_dir.citPA2017[data_dir.citPA2017 > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citPA2017 > 10.10 AND cit2016 > 117378.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Package version\n",
    "updated_PBC4CIP = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target attribute\n",
    "attname = 'Forma del procedimiento'\n",
    "attnum = 2\n",
    "filename = attname + str(attnum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with the nominal attributes in order, and their respective onehot indexes\n",
    "# Only change this if an attribute is moved or deleted\n",
    "\"\"\" nom_pos = {\n",
    "    'Caracter del procedimiento' : [18,19],\n",
    "    'Forma del procedimiento' : [7,8,9],\n",
    "    'Operador' : [],\n",
    "    'Correo electronico' : [],\n",
    "    'Entidad federativa' : [10,11,12,13,14,15],\n",
    "    'Tipo de contratación' : [21,22,23,24,25],\n",
    "    'Articulo' : [18,19,20],\n",
    "    'Plantilla' : [26,27,28,29,30,31]\n",
    "} \"\"\"\n",
    "nom_pos = {\n",
    "    'Caracter del procedimiento' : [18,19],\n",
    "    'Forma del procedimiento' : [9,10,11],\n",
    "    'Operador' : [],\n",
    "    'Correo electronico' : [],\n",
    "    'Entidad federativa' : [12,13,14,15,16,17],\n",
    "    'Tipo de contratación' : [23,24,25,26,27],\n",
    "    'Articulo' : [20,22,22],\n",
    "    'Plantilla' : [28,29,30,31,32,33]\n",
    "}\n",
    "class_index = len(nom_pos.keys()) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{&#39;Caracter del procedimiento&#39;: [18, 19],\n &#39;Forma del procedimiento&#39;: [9, 10, 11],\n &#39;Operador&#39;: [],\n &#39;Correo electronico&#39;: [],\n &#39;Entidad federativa&#39;: [12, 13, 14, 15, 16, 17],\n &#39;Tipo de contratación&#39;: [23, 24, 25, 26, 27],\n &#39;Articulo&#39;: [20, 22, 22],\n &#39;Plantilla&#39;: [28, 29, 30, 31, 32, 33]}"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "nom_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = 'dany.pickle'\n",
    "load_pickle = True\n",
    "save_pickle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_pickle:\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        pickle.dump([   wekafiles_path,\n",
    "                        PBC4CIP_zip_path,\n",
    "                        data_dir,\n",
    "                        arff_file,\n",
    "                        updated_PBC4CIP], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_pickle:\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        wekafiles_path, PBC4CIP_zip_path, data_dir, arff_file, updated_PBC4CIP = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "DEBUG:weka.core.jvm:Adding bundled jars\nDEBUG:weka.core.jvm:Classpath=[&#39;/Users/dannygc/anaconda3/lib/python3.7/site-packages/javabridge/jars/rhino-1.7R4.jar&#39;, &#39;/Users/dannygc/anaconda3/lib/python3.7/site-packages/javabridge/jars/runnablequeue.jar&#39;, &#39;/Users/dannygc/anaconda3/lib/python3.7/site-packages/javabridge/jars/cpython.jar&#39;, &#39;/Users/dannygc/anaconda3/lib/python3.7/site-packages/weka/lib/python-weka-wrapper.jar&#39;, &#39;/Users/dannygc/anaconda3/lib/python3.7/site-packages/weka/lib/weka.jar&#39;]\nDEBUG:weka.core.jvm:MaxHeapSize=12g\nDEBUG:weka.core.jvm:Using alternative Weka home directory: /Users/dannygc/wekafiles\nDEBUG:weka.core.jvm:Using alternative Weka home directory: /Users/dannygc/wekafiles\n"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#jvm.start(packages=True, max_heap_size=\"12g\") #max_heap_size 512m, 4g. packages=true searches for weka packages in installation program\n",
    "jvm.start(packages=wekafiles_path, max_heap_size='12g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "weka.classifiers.trees.PBC4cip\nPBC4cip is already installed.\n"
    }
   ],
   "source": [
    "pkg = \"PBC4cip\"\n",
    "print(complete_classname(\".\" + pkg))\n",
    "# install package if necessary \n",
    "if not packages.is_installed(pkg):\n",
    "    print(\"Installing %s...\" % pkg)\n",
    "    #packages.install_package(\"http://prdownloads.sourceforge.net/weka/discriminantAnalysis1.0.3.zip?download\")\n",
    "    packages.install_package(PBC4CIP_zip_path)\n",
    "    print(\"Installed %s, please re-run script!\" % pkg)\n",
    "    jvm.stop()\n",
    "    sys.exit(0)\n",
    "else:\n",
    "    print(pkg + \" is already installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n\n\n\n\n\n&gt;&gt;&gt; Start...\n"
    }
   ],
   "source": [
    "# testing classname completion\n",
    "\n",
    "print(\"\\n\\n\\n\\n\\n\")\n",
    "print(\">>> Start...\")\n",
    "\n",
    "loader = Loader(classname=\"weka.core.converters.ArffLoader\")\n",
    "data = loader.load_file(data_dir + arff_file)\n",
    "\n",
    "cmdline = []"
   ]
  },
  {
   "source": [
    "### Preprocessing\n",
    "Here the one-hot attributes not used are deleted.\n",
    "Also the nominal attribute where the class being evaluated comes from is also deleted."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[&#39;Caracter del procedimiento&#39;,\n &#39;Forma del procedimiento&#39;,\n &#39;Operador&#39;,\n &#39;Correo electronico&#39;,\n &#39;Entidad federativa&#39;,\n &#39;Tipo de contratacion&#39;,\n &#39;Articulo&#39;,\n &#39;Plantilla&#39;,\n &#39;Forma_Procedimiento_Presencial&#39;,\n &#39;Forma_Procedimiento_Electronica&#39;,\n &#39;Forma_Procedimiento_Mixta&#39;,\n &#39;Entidad_federativa_Norte&#39;,\n &#39;Entidad_federativa_Centro&#39;,\n &#39;Entidad_federativa_Golfo&#39;,\n &#39;Entidad_federativa_PacificoSur&#39;,\n &#39;Entidad_federativa_Sur&#39;,\n &#39;Entidad_federativa_PacificoNorte&#39;,\n &#39;Caracter del procedimiento_Nacional&#39;,\n &#39;Caracter del procedimiento_Internacional&#39;,\n &#39;Articulo de excepcion_41&#39;,\n &#39;Articulo de excepcion_42&#39;,\n &#39;Articulo de excepcion_43&#39;,\n &#39;Contratacion_adquisiciones&#39;,\n &#39;Contratacion_servicios&#39;,\n &#39;Contratacion_obra_publica&#39;,\n &#39;Contratacion_arrendamientos&#39;,\n &#39;Contratacion_servicios_op&#39;,\n &#39;Plantilla_Proyecto&#39;,\n &#39;Plantilla_Adjudicación&#39;,\n &#39;Plantilla_Licitación&#39;,\n &#39;Plantilla_Invitación&#39;,\n &#39;Plantilla_Reporte&#39;,\n &#39;Plantilla_Contratos&#39;,\n &#39;Most_used_description_word_servici&#39;,\n &#39;Most_used_description_word_adquisicion&#39;,\n &#39;Most_used_description_word_material&#39;,\n &#39;Mes_pub&#39;,\n &#39;Dia_pub&#39;,\n &#39;Hora_pub&#39;,\n &#39;Most_used_UC_word_gyr&#39;,\n &#39;Most_used_UC_word_material&#39;,\n &#39;Most_used_UC_word_general&#39;]"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "attributes = [f.name for f in data.attributes()]\n",
    "attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_vector = [False for i in range(len(attributes))]\n",
    "pos_vector[0:len(nom_pos.keys())] = [True for i in range(len(nom_pos.keys()))]\n",
    "pos_vector[len(attributes)-9:len(attributes)] = [True for i in range(9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dict that will contain filtered datasets\n",
    "filtered_data = {\n",
    "    'Caracter del procedimiento' : [],\n",
    "    'Forma del procedimiento' : [],\n",
    "    'Entidad federativa' : [],\n",
    "    'Tipo de contratación' : [],\n",
    "    'Articulo' : [],\n",
    "    'Plantilla' : []\n",
    "}\n",
    "for key, value in nom_pos.items():\n",
    "    for onehot_att in value:\n",
    "        # Here modify pos vector then feed it into weka remove\n",
    "        # Change nom att to false\n",
    "        # Change only specific onehot to true\n",
    "        # See how to save it to change the class also\n",
    "        nominal_att_index = list(nom_pos.keys()).index(key)\n",
    "        pos_vector[nominal_att_index] = False\n",
    "        pos_vector[onehot_att-1] = True\n",
    "        indeces_not_filtered = [i+1 for i, val in enumerate(pos_vector) if val]\n",
    "        pos_vector[nominal_att_index] = True\n",
    "        pos_vector[onehot_att-1] = False\n",
    "        remove = Filter(classname=\"weka.filters.unsupervised.attribute.Remove\", options=[\"-R\",\",\".join(map(str, indeces_not_filtered)),\"-V\"])\n",
    "        remove.inputformat(data)\n",
    "        filtered_data[key].append(remove.filter(data))\n",
    "            "
   ]
  },
  {
   "source": [
    "## Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "trees = 150\n",
    "maxDepth = 5\n",
    "min_objectsByLeaf = 35\n",
    "#Define Constraints\n",
    "min_support_diff = 0.3 # ['s0-s1']\n",
    "min_count_diff = 30 #['c0-c1']\n",
    "min_ratio = 0.6 #['c0/(c0 +c1)'] \n",
    "max_patterns = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Target Attribute is: Forma_Procedimiento_Mixta\nMax Depth is: 5\nNumber of Trees is: 150\nMin Objects by Leaf is: 35\n"
    }
   ],
   "source": [
    "data = filtered_data[attname][attnum]\n",
    "data.class_index = class_index\n",
    "\n",
    "print(\"Target Attribute is: \" + [f.name for f in data.attributes()][class_index])\n",
    "print(f'Max Depth is: {maxDepth}')\n",
    "print(f'Number of Trees is: {trees}')\n",
    "print(f'Min Objects by Leaf is: {min_objectsByLeaf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "&gt;&gt;&gt; Building classifier...\n&gt;&gt;&gt; [Done] Bulding classifier. Time: 375.1266739368439 seconds ---\n&gt;&gt;&gt; Serializing model...\n&gt;&gt;&gt; [Done] Serializing model. Time: 0.7039411067962646 seconds ---\n&gt;&gt;&gt; Generating big string...\n&gt;&gt;&gt; [Done] Generating big string. Time: 0.16977787017822266 seconds ---\n"
    }
   ],
   "source": [
    "\n",
    "\n",
    "input_config = f'weka.classifiers.trees.PBC4cip -S 1 -filter -miner \\\"PRFramework.Core.SupervisedClassifiers.EmergingPatterns.Miners.RandomForestMinerWithoutFiltering -bagSizePercent 100 -numFeatures -1 -numTrees {trees} -builder \\\\\\\"PRFramework.Core.SupervisedClassifiers.DecisionTrees.Builder.DecisionTreeBuilder -distributionEvaluator \\\\\\\\\\\\\\\"PRFramework.Core.SupervisedClassifiers.DecisionTrees.DistributionEvaluators.Hellinger \\\\\\\\\\\\\\\" -maxDepth {maxDepth} \\\\\\\\\\\\\\\"-minimalObjByLeaf \\\\\\\\\\\\\\\" {min_objectsByLeaf} -minimalSplitGain 1.0E-30\\\\\\\"\\\"'\n",
    "\n",
    "\n",
    "#weka.classifiers.trees.PBC4cip -S 1 -filter -miner \"PRFramework.Core.SupervisedClassifiers.EmergingPatterns.Miners.RandomForestMinerWithoutFiltering -bagSizePercent 100 -numFeatures -1 -numTrees 150 -builder \\\"PRFramework.Core.SupervisedClassifiers.DecisionTrees.Builder.DecisionTreeBuilder -distributionEvaluator \\\\\\\"PRFramework.Core.SupervisedClassifiers.DecisionTrees.DistributionEvaluators.Hellinger \\\\\\\" -maxDepth 5 \\\\\\\"-minimalObjByLeaf \\\\\\\" 35 -minimalSplitGain 1.0E-30\\\"\"\n",
    "\n",
    "\n",
    "classifier = from_commandline(input_config, classname=\"weka.classifiers.Classifier\")\n",
    "\n",
    "print(\">>> Building classifier...\")\n",
    "start_time_1 = time.time()\n",
    "classifier.build_classifier(data)\n",
    "print(f\">>> [Done] Bulding classifier. Time: {(time.time() - start_time_1)} seconds ---\")\n",
    "\n",
    "print(\">>> Serializing model...\")\n",
    "start_time_1 = time.time()\n",
    "classifier.serialize(f\"{filename}.model\")\n",
    "print(f\">>> [Done] Serializing model. Time: {(time.time() - start_time_1)} seconds ---\")\n",
    "\n",
    "print(\">>> Generating big string...\")\n",
    "start_time_1 = time.time()\n",
    "big_string = str(classifier).split(\"]\\n\")\n",
    "print(f\">>> [Done] Generating big string. Time: {(time.time() - start_time_1)} seconds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "c0_count:  86888\nc1_count:  9234\nTotal:  96122\n"
    }
   ],
   "source": [
    "c0_count = 0\n",
    "c1_count = 0\n",
    "\n",
    "for instance in data:\n",
    "    if instance.values[class_index] == 0:\n",
    "        c0_count += 1\n",
    "    else:\n",
    "        c1_count += 1\n",
    "print('c0_count: ', c0_count)\n",
    "print('c1_count: ', c1_count)\n",
    "print('Total: ', c0_count + c1_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 965/965 [00:00&lt;00:00, 224773.89it/s]\n"
    }
   ],
   "source": [
    "list_fields = []\n",
    "list_support_c0 = []\n",
    "list_support_c1 = []\n",
    "list_num_c0 = []\n",
    "list_num_c1 = []\n",
    "list_diff = []\n",
    "with tqdm(total=len(big_string)) as pbar:\n",
    "    for item in big_string:\n",
    "        text = \"\"\n",
    "        text = item + \"]\"\n",
    "        text = text.split(\"[\")\n",
    "        fields = text[0]\n",
    "        # print(fields)\n",
    "        if updated_PBC4CIP:\n",
    "            if (len(text) > 1):\n",
    "                class_nums = text[1]\n",
    "                class_nums = class_nums[:-1]\n",
    "                class_nums = class_nums.split()\n",
    "                supports = text[2]\n",
    "                supports = supports[:-1]\n",
    "                supports = supports.split()\n",
    "                list_fields.append(fields.strip())\n",
    "                list_support_c0.append(float(supports[0]))\n",
    "                list_support_c1.append(float(supports[1]))\n",
    "                list_num_c0.append(float(class_nums[0]))\n",
    "                list_num_c1.append(float(class_nums[1]))\n",
    "        else:\n",
    "            if (len(text) > 1):\n",
    "                supports = text[1]\n",
    "                supports = supports[:-1]\n",
    "                supports = supports.split()\n",
    "                list_fields.append(fields.strip())\n",
    "                list_support_c0.append(float(supports[0]))\n",
    "                list_support_c1.append(float(supports[1]))\n",
    "                list_num_c0.append(float(supports[0])*c0_count)\n",
    "                list_num_c1.append(float(supports[1])*c1_count)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "source": [
    "## Results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                              patterns  support_c0  \\\n959  Operador != &#39;juan ramon garcia lopez&#39; AND Plan...        0.73   \n960  Correo electronico != &#39;diconsa&#39; AND Operador !...        0.80   \n961  Correo electronico != &#39;diconsa&#39; AND Correo ele...        0.80   \n962  Correo electronico != &#39;diconsa&#39; AND Correo ele...        0.78   \n963  Operador != &#39;juan ramon garcia lopez&#39; AND Plan...        0.78   \n\n     support_c1    num_c0   num_c1  \n959        0.93  63428.24  8587.62  \n960        0.94  69510.40  8679.96  \n961        0.94  69510.40  8679.96  \n962        0.97  67772.64  8956.98  \n963        0.97  67772.64  8956.98  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>patterns</th>\n      <th>support_c0</th>\n      <th>support_c1</th>\n      <th>num_c0</th>\n      <th>num_c1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>959</th>\n      <td>Operador != 'juan ramon garcia lopez' AND Plan...</td>\n      <td>0.73</td>\n      <td>0.93</td>\n      <td>63428.24</td>\n      <td>8587.62</td>\n    </tr>\n    <tr>\n      <th>960</th>\n      <td>Correo electronico != 'diconsa' AND Operador !...</td>\n      <td>0.80</td>\n      <td>0.94</td>\n      <td>69510.40</td>\n      <td>8679.96</td>\n    </tr>\n    <tr>\n      <th>961</th>\n      <td>Correo electronico != 'diconsa' AND Correo ele...</td>\n      <td>0.80</td>\n      <td>0.94</td>\n      <td>69510.40</td>\n      <td>8679.96</td>\n    </tr>\n    <tr>\n      <th>962</th>\n      <td>Correo electronico != 'diconsa' AND Correo ele...</td>\n      <td>0.78</td>\n      <td>0.97</td>\n      <td>67772.64</td>\n      <td>8956.98</td>\n    </tr>\n    <tr>\n      <th>963</th>\n      <td>Operador != 'juan ramon garcia lopez' AND Plan...</td>\n      <td>0.78</td>\n      <td>0.97</td>\n      <td>67772.64</td>\n      <td>8956.98</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "df = pd.DataFrame(columns = ['patterns', 'support_c0', 'support_c1', 'num_c0', 'num_c1'])\n",
    "df['patterns']=list_fields\n",
    "df['support_c0']=list_support_c0\n",
    "df['support_c1']= list_support_c1\n",
    "df['num_c0']= list_num_c0\n",
    "df['num_c1']= list_num_c1\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Empty DataFrame\nColumns: [patterns, support_c0, support_c1, num_c0, num_c1, s0-s1, c0-c1, c0/(c0 +c1)]\nIndex: []",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>patterns</th>\n      <th>support_c0</th>\n      <th>support_c1</th>\n      <th>num_c0</th>\n      <th>num_c1</th>\n      <th>s0-s1</th>\n      <th>c0-c1</th>\n      <th>c0/(c0 +c1)</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "df['s0-s1'] = df.support_c1 - df.support_c0\n",
    "df['c0-c1'] = df.num_c1 - df.num_c0\n",
    "df['c0/(c0 +c1)'] = df.num_c1/(df.num_c0 + df.num_c1)\n",
    "\n",
    "df = df[df['s0-s1'] > min_support_diff]\n",
    "df = df[df['c0-c1'] > min_count_diff]\n",
    "df = df[df['c0/(c0 +c1)'] > min_ratio]\n",
    "\n",
    "df = df.sort_values(by=['s0-s1'], ascending=False)\n",
    "df = df[:max_patterns]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "source": [
    "### Export to CSV"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"Patterns_Obtained/{filename}.csv\", index=False)\n",
    "jvm.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}