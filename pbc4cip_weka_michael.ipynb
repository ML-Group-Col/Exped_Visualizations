{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.6 64-bit ('base': conda)",
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "f515a261c91f2d540a32fd338636eea76c71c483334d8e72b3eaaa34365c9198"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Pbc4cip- Python Implementation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import weka.core.jvm as jvm\n",
    "import weka.core.packages as packages\n",
    "from weka.core.classes import complete_classname\n",
    "from weka.core.converters import Loader\n",
    "from weka.classifiers import Classifier\n",
    "import weka.plot.graph as graph  # NB: pygraphviz and PIL are required\n",
    "from weka.core.classes import Random, from_commandline\n",
    "import weka.core.serialization as serialization\n",
    "from weka.filters import Filter\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User specified - Dany\n",
    "wekafiles_path = \"/Users/dannygc/wekafiles\"\n",
    "PBC4CIP_zip_path = \"/Users/dannygc/Google Drive File Stream/My Drive/MCCNotes/MCC 3/Adv_ML/PBC_HW4/PBC4cip-1.0-weka.zip\"\n",
    "data_dir = \"/Volumes/GoogleDrive/My Drive/MCCNotes/Jlab projects/GITHUB_repositories/DRAE_repositories/Exped_Visualizations/\"\n",
    "arff_file = \"weka_dataset_clean_.arff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User specified - Michael\n",
    "#wekafiles_path = \"C:/Users/mzenk/wekafiles\"\n",
    "#PBC4CIP_zip_path = \"C:/Users/mzenk/Google Drive/ITESM/Maestría/Semestre 3/ML2/Assignment4/PBC4cip-1.0-weka.zip\"\n",
    "#data_dir = \"C:/Users/mzenk/Google Drive/ITESM/Maestría/Semestre 3/ML2/Assignment4/Exped_Visualizations/\"\n",
    "#arff_file = \"weka_dataset_clean_.arff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criteria for Patterns\n",
    "updated_PBC4CIP = False\n",
    "min_support_diff = 0.3\n",
    "min_count_diff = 30\n",
    "min_ratio = 0.6\n",
    "max_patterns = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "trees = 150\n",
    "maxDepth = 5\n",
    "objectsByLeaf = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "attname = 'Forma del procedimiento'\n",
    "attnum = 0\n",
    "filename = attname + str(attnum)\n",
    "\n",
    "# Dictionary with the nominal attributes in order, and their respective onehot indexes\n",
    "# Only change this if an attribute is moved or deleted\n",
    "nom_pos = {\n",
    "    'Caracter del procedimiento' : [18,19],\n",
    "    'Forma del procedimiento' : [7,8,9],\n",
    "    'Operador' : [],\n",
    "    'Correo electronico' : [],\n",
    "    'Entidad federativa' : [10,11,12,13,14,15],\n",
    "    'Tipo de contratación' : [21,22,23,24,25],\n",
    "    'Articulo' : [18,19,20],\n",
    "    'Plantilla' : [26,27,28,29,30,31]\n",
    "}\n",
    "class_index = len(nom_pos.keys())-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = 'dany.pickle'\n",
    "load_pickle = True\n",
    "save_pickle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_pickle:\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        pickle.dump([   wekafiles_path,\n",
    "                        PBC4CIP_zip_path,\n",
    "                        data_dir,\n",
    "                        arff_file,\n",
    "                        updated_PBC4CIP], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_pickle:\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        wekafiles_path, PBC4CIP_zip_path, data_dir, arff_file, updated_PBC4CIP = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "DEBUG:weka.core.jvm:Adding bundled jars\nDEBUG:weka.core.jvm:Classpath=[&#39;/Users/dannygc/anaconda3/lib/python3.7/site-packages/javabridge/jars/rhino-1.7R4.jar&#39;, &#39;/Users/dannygc/anaconda3/lib/python3.7/site-packages/javabridge/jars/runnablequeue.jar&#39;, &#39;/Users/dannygc/anaconda3/lib/python3.7/site-packages/javabridge/jars/cpython.jar&#39;, &#39;/Users/dannygc/anaconda3/lib/python3.7/site-packages/weka/lib/python-weka-wrapper.jar&#39;, &#39;/Users/dannygc/anaconda3/lib/python3.7/site-packages/weka/lib/weka.jar&#39;]\nDEBUG:weka.core.jvm:MaxHeapSize=12g\nDEBUG:weka.core.jvm:Using alternative Weka home directory: /Users/dannygc/wekafiles\nDEBUG:weka.core.jvm:Using alternative Weka home directory: /Users/dannygc/wekafiles\n"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#jvm.start(packages=True, max_heap_size=\"12g\") #max_heap_size 512m, 4g. packages=true searches for weka packages in installation program\n",
    "jvm.start(packages=wekafiles_path, max_heap_size='12g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "weka.classifiers.trees.PBC4cip\nPBC4cip is already installed.\n"
    }
   ],
   "source": [
    "pkg = \"PBC4cip\"\n",
    "print(complete_classname(\".\" + pkg))\n",
    "# install package if necessary \n",
    "if not packages.is_installed(pkg):\n",
    "    print(\"Installing %s...\" % pkg)\n",
    "    #packages.install_package(\"http://prdownloads.sourceforge.net/weka/discriminantAnalysis1.0.3.zip?download\")\n",
    "    packages.install_package(PBC4CIP_zip_path)\n",
    "    print(\"Installed %s, please re-run script!\" % pkg)\n",
    "    jvm.stop()\n",
    "    sys.exit(0)\n",
    "else:\n",
    "    print(pkg + \" is already installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n\n\n\n\n\n&gt;&gt;&gt; Start...\n"
    }
   ],
   "source": [
    "# testing classname completion\n",
    "\n",
    "print(\"\\n\\n\\n\\n\\n\")\n",
    "print(\">>> Start...\")\n",
    "\n",
    "loader = Loader(classname=\"weka.core.converters.ArffLoader\")\n",
    "data = loader.load_file(data_dir + arff_file)\n",
    "\n",
    "cmdline = []"
   ]
  },
  {
   "source": [
    "# Preprocess\n",
    "Here the one-hot attributes not used are deleted.\n",
    "Also the nominal attribute where the class being evaluated comes from is also deleted."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pos_vector = [False for i in range(43)]\n",
    "pos_vector[0:8] = [True for i in range(8)]\n",
    "pos_vector[33:43] = [True for i in range(10)]\n",
    "\n",
    "# Dict that will contain filtered datasets\n",
    "filtered_data = {\n",
    "    'Caracter del procedimiento' : [],\n",
    "    'Forma del procedimiento' : [],\n",
    "    'Entidad federativa' : [],\n",
    "    'Tipo de contratación' : [],\n",
    "    'Articulo' : [],\n",
    "    'Plantilla' : []\n",
    "}\n",
    "for key, value in nom_pos.items():\n",
    "    for onehot_att in value:\n",
    "        # Here modify pos vector then feed it into weka remove\n",
    "        # Change nom att to false\n",
    "        # Change only specific onehot to true\n",
    "        # See how to save it to change the class also\n",
    "        nominal_att_index = list(nom_pos.keys()).index(key)\n",
    "        pos_vector[nominal_att_index] = False\n",
    "        pos_vector[onehot_att-1] = True\n",
    "        indeces_not_filtered = [i+1 for i, val in enumerate(pos_vector) if val]\n",
    "        pos_vector[nominal_att_index] = True\n",
    "        pos_vector[onehot_att-1] = False\n",
    "        remove = Filter(classname=\"weka.filters.unsupervised.attribute.Remove\", options=[\"-R\",\",\".join(map(str, indeces_not_filtered)),\"-V\"])\n",
    "        remove.inputformat(data)\n",
    "        filtered_data[key].append(remove.filter(data))\n",
    "            "
   ]
  },
  {
   "source": [
    "# Criterions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "&gt;&gt;&gt; Building classifier...\n&gt;&gt;&gt; [Done] Bulding classifier. Time: 288.4211390018463 seconds ---\n&gt;&gt;&gt; Serializing model...\n&gt;&gt;&gt; [Done] Serializing model. Time: 1.1611230373382568 seconds ---\n&gt;&gt;&gt; Generating big string...\n&gt;&gt;&gt; [Done] Generating big string. Time: 1.1709182262420654 seconds ---\n"
    }
   ],
   "source": [
    "data = filtered_data[attname][attnum]\n",
    "data.class_index = class_index\n",
    "\n",
    "input_config = f'weka.classifiers.trees.PBC4cip -S 1 -miner \\\"PRFramework.Core.SupervisedClassifiers.EmergingPatterns.Miners.RandomForestMinerWithoutFiltering -bagSizePercent 100 -numFeatures -1 -numTrees {trees} -builder \\\\\\\"PRFramework.Core.SupervisedClassifiers.DecisionTrees.Builder.DecisionTreeBuilder -distributionEvaluator \\\\\\\\\\\\\\\"PRFramework.Core.SupervisedClassifiers.DecisionTrees.DistributionEvaluators.QuinlanGain \\\\\\\\\\\\\\\" -maxDepth {maxDepth} \\\\\\\\\\\\\\\"-minimalObjByLeaf \\\\\\\\\\\\\\\" {objectsByLeaf} -minimalSplitGain 1.0E-30\\\\\\\"\\\"'\n",
    "\n",
    "classifier = from_commandline(input_config, classname=\"weka.classifiers.Classifier\")\n",
    "\n",
    "print(\">>> Building classifier...\")\n",
    "start_time_1 = time.time()\n",
    "classifier.build_classifier(data)\n",
    "print(f\">>> [Done] Bulding classifier. Time: {(time.time() - start_time_1)} seconds ---\")\n",
    "\n",
    "print(\">>> Serializing model...\")\n",
    "start_time_1 = time.time()\n",
    "classifier.serialize(f\"{filename}.model\")\n",
    "print(f\">>> [Done] Serializing model. Time: {(time.time() - start_time_1)} seconds ---\")\n",
    "\n",
    "print(\">>> Generating big string...\")\n",
    "start_time_1 = time.time()\n",
    "big_string = str(classifier).split(\"]\\n\")\n",
    "print(f\">>> [Done] Generating big string. Time: {(time.time() - start_time_1)} seconds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "C0_count: 68902\nC1_count: 27220\nTotal: 96122\n"
    }
   ],
   "source": [
    "attributes = []\n",
    "c0_count = 0\n",
    "c1_count = 0\n",
    "\n",
    "for instance in data:\n",
    "    if instance.values[class_index] == 0:\n",
    "        c0_count += 1\n",
    "    else:\n",
    "        c1_count += 1\n",
    "print('C0_count:', c0_count)\n",
    "print('C1_count:', c1_count)\n",
    "print('Total:', c0_count + c1_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 2200/2200 [00:00&lt;00:00, 245483.22it/s]\n"
    }
   ],
   "source": [
    "list_fields = []\n",
    "list_support_c0 = []\n",
    "list_support_c1 = []\n",
    "list_num_c0 = []\n",
    "list_num_c1 = []\n",
    "list_diff = []\n",
    "with tqdm(total=len(big_string)) as pbar:\n",
    "    for item in big_string:\n",
    "        text = \"\"\n",
    "        text = item + \"]\"\n",
    "        text = text.split(\"[\")\n",
    "        fields = text[0]\n",
    "        # print(fields)\n",
    "        if updated_PBC4CIP:\n",
    "            if (len(text) > 1):\n",
    "                class_nums = text[1]\n",
    "                class_nums = class_nums[:-1]\n",
    "                class_nums = class_nums.split()\n",
    "                supports = text[2]\n",
    "                supports = supports[:-1]\n",
    "                supports = supports.split()\n",
    "                list_fields.append(fields.strip())\n",
    "                list_support_c0.append(float(supports[0]))\n",
    "                list_support_c1.append(float(supports[1]))\n",
    "                list_num_c0.append(float(class_nums[0]))\n",
    "                list_num_c1.append(float(class_nums[1]))\n",
    "        else:\n",
    "            if (len(text) > 1):\n",
    "                supports = text[1]\n",
    "                supports = supports[:-1]\n",
    "                supports = supports.split()\n",
    "                list_fields.append(fields.strip())\n",
    "                list_support_c0.append(float(supports[0]))\n",
    "                list_support_c1.append(float(supports[1]))\n",
    "                list_num_c0.append(float(supports[0])*c0_count)\n",
    "                list_num_c1.append(float(supports[1])*c1_count)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                              fields  support_c0  support_c1  \\\n0  Entidad federativa = &#39;centro&#39; AND Tipo de cont...         0.0         0.0   \n1  Articulo = &#39;43&#39; AND Correo electronico = &#39;espa...         0.0         0.0   \n2  Correo electronico = &#39;diconsa&#39; AND Operador = ...         0.0         0.0   \n3  Correo electronico = &#39;diconsa&#39; AND Most_used_U...         0.0         0.0   \n4  Caracter del procedimiento != &#39;nacional&#39; AND T...         0.0         0.0   \n\n   num_c0  num_c1  \n0     0.0     0.0  \n1     0.0     0.0  \n2     0.0     0.0  \n3     0.0     0.0  \n4     0.0     0.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fields</th>\n      <th>support_c0</th>\n      <th>support_c1</th>\n      <th>num_c0</th>\n      <th>num_c1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Entidad federativa = 'centro' AND Tipo de cont...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Articulo = '43' AND Correo electronico = 'espa...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Correo electronico = 'diconsa' AND Operador = ...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Correo electronico = 'diconsa' AND Most_used_U...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Caracter del procedimiento != 'nacional' AND T...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "df = pd.DataFrame(columns = ['fields', 'support_c0', 'support_c1', 'num_c0', 'num_c1'])\n",
    "df['fields']=list_fields\n",
    "df['support_c0']=list_support_c0\n",
    "df['support_c1']= list_support_c1\n",
    "df['num_c0']= list_num_c0\n",
    "df['num_c1']= list_num_c1\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                 fields  support_c0  \\\n2198  Most_used_description_word_adquisicion = &#39;0&#39; A...        0.17   \n2197  Caracter del procedimiento = &#39;nacional&#39; AND Co...        0.16   \n2195  Tipo de contratacion = &#39;servicios&#39; AND Caracte...        0.14   \n2189  Tipo de contratacion = &#39;servicios&#39; AND Articul...        0.14   \n2192  Tipo de contratacion = &#39;servicios&#39; AND Articul...        0.14   \n\n      support_c1    num_c0   num_c1  s_diff    n_diff     ratio  \n2198        0.90  11713.34  24498.0    0.73  12784.66  0.676528  \n2197        0.88  11024.32  23953.6    0.72  12929.28  0.684821  \n2195        0.82   9646.28  22320.4    0.68  12674.12  0.698240  \n2189        0.81   9646.28  22048.2    0.67  12401.92  0.695648  \n2192        0.81   9646.28  22048.2    0.67  12401.92  0.695648  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fields</th>\n      <th>support_c0</th>\n      <th>support_c1</th>\n      <th>num_c0</th>\n      <th>num_c1</th>\n      <th>s_diff</th>\n      <th>n_diff</th>\n      <th>ratio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2198</th>\n      <td>Most_used_description_word_adquisicion = '0' A...</td>\n      <td>0.17</td>\n      <td>0.90</td>\n      <td>11713.34</td>\n      <td>24498.0</td>\n      <td>0.73</td>\n      <td>12784.66</td>\n      <td>0.676528</td>\n    </tr>\n    <tr>\n      <th>2197</th>\n      <td>Caracter del procedimiento = 'nacional' AND Co...</td>\n      <td>0.16</td>\n      <td>0.88</td>\n      <td>11024.32</td>\n      <td>23953.6</td>\n      <td>0.72</td>\n      <td>12929.28</td>\n      <td>0.684821</td>\n    </tr>\n    <tr>\n      <th>2195</th>\n      <td>Tipo de contratacion = 'servicios' AND Caracte...</td>\n      <td>0.14</td>\n      <td>0.82</td>\n      <td>9646.28</td>\n      <td>22320.4</td>\n      <td>0.68</td>\n      <td>12674.12</td>\n      <td>0.698240</td>\n    </tr>\n    <tr>\n      <th>2189</th>\n      <td>Tipo de contratacion = 'servicios' AND Articul...</td>\n      <td>0.14</td>\n      <td>0.81</td>\n      <td>9646.28</td>\n      <td>22048.2</td>\n      <td>0.67</td>\n      <td>12401.92</td>\n      <td>0.695648</td>\n    </tr>\n    <tr>\n      <th>2192</th>\n      <td>Tipo de contratacion = 'servicios' AND Articul...</td>\n      <td>0.14</td>\n      <td>0.81</td>\n      <td>9646.28</td>\n      <td>22048.2</td>\n      <td>0.67</td>\n      <td>12401.92</td>\n      <td>0.695648</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "df['s_diff'] = df.support_c1 - df.support_c0\n",
    "df['n_diff'] = df.num_c1 - df.num_c0\n",
    "df['ratio'] = df.num_c1/(df.num_c0 + df.num_c1)\n",
    "\n",
    "df = df[df.s_diff > min_support_diff]\n",
    "df = df[df.n_diff > min_count_diff]\n",
    "df = df[df.ratio > min_ratio]\n",
    "\n",
    "df = df.sort_values(by=['s_diff'], ascending=False)\n",
    "df = df[:max_patterns]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "source": [
    "## Export to Patterns Obtained"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"Patterns_Obtained/{filename}.csv\", index=False)\n",
    "jvm.stop()"
   ]
  }
 ]
}