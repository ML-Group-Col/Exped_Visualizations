{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.6 64-bit ('base': conda)",
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "f515a261c91f2d540a32fd338636eea76c71c483334d8e72b3eaaa34365c9198"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import weka.core.jvm as jvm\n",
    "import weka.core.packages as packages\n",
    "from weka.core.classes import complete_classname\n",
    "from weka.core.converters import Loader\n",
    "from weka.classifiers import Classifier\n",
    "import weka.plot.graph as graph  # NB: pygraphviz and PIL are required\n",
    "from weka.core.classes import Random, from_commandline\n",
    "import weka.core.serialization as serialization\n",
    "from weka.filters import Filter\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "wekafiles_path = \"C:/Users/mzenk/wekafiles\"\n",
    "PBC4CIP_zip_path = \"C:/Users/mzenk/Google Drive/ITESM/Maestría/Semestre 3/ML2/Assignment4/PBC4cip-1.0-weka.zip\"\n",
    "data_dir = \"C:/Users/mzenk/Google Drive/ITESM/Maestría/Semestre 3/ML2/Assignment4/Exped_Visualizations/\"\n",
    "arff_file = \"weka_dataset_clean.arff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:weka.core.jvm:JVM already running, call jvm.stop() first\n"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#jvm.start(packages=True, max_heap_size=\"12g\") #max_heap_size 512m, 4g. packages=true searches for weka packages in installation program\n",
    "jvm.start(packages=wekafiles_path, max_heap_size='12g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "weka.classifiers.trees.PBC4cip\nPBC4cip is already installed.\n"
    }
   ],
   "source": [
    "pkg = \"PBC4cip\"\n",
    "print(complete_classname(\".\" + pkg))\n",
    "# install package if necessary \n",
    "if not packages.is_installed(pkg):\n",
    "    print(\"Installing %s...\" % pkg)\n",
    "    #packages.install_package(\"http://prdownloads.sourceforge.net/weka/discriminantAnalysis1.0.3.zip?download\")\n",
    "    packages.install_package(PBC4CIP_zip_path)\n",
    "    print(\"Installed %s, please re-run script!\" % pkg)\n",
    "    jvm.stop()\n",
    "    sys.exit(0)\n",
    "else:\n",
    "    print(pkg + \" is already installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n\n\n\n\n\n&gt;&gt;&gt; Start...\n"
    }
   ],
   "source": [
    "# testing classname completion\n",
    "\n",
    "print(\"\\n\\n\\n\\n\\n\")\n",
    "print(\">>> Start...\")\n",
    "\n",
    "loader = Loader(classname=\"weka.core.converters.ArffLoader\")\n",
    "data = loader.load_file(data_dir + arff_file)\n",
    "data.class_is_last()\n",
    "\n",
    "filename_array = [\"NumberOfTrees\", \"RandomFeatures\", \"DepthOfTrees\", \"MinimumObjectByLeaf\", \"RandomSubSpace\", \"InfoGainAttributeEval\", \"PrincipalComponents\", \"GainRatioAttributeEval\", \"WrapperSubsetEval\", \"CorrelationAttributeEval\"]\n",
    "filename = str(filename_array[9])\n",
    "filename_index = filename_array.index(filename)\n",
    "\n",
    "cmdline = []"
   ]
  },
  {
   "source": [
    "# Preprocess\n",
    "Here the one-hot attributes not used are deleted.\n",
    "Also the nominal attribute where the class being evaluated comes from is also deleted."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(&#39;d&#39;, &#39;e&#39;)\n"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[True,\n True,\n True,\n True,\n True,\n True,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True]"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "pos_vector = [False for i in range(41)]\n",
    "pos_vector[0:6] = [True for i in range(6)]\n",
    "pos_vector[31:41] = [True for i in range(10)]\n",
    "# Dictionary with the nominal attributes in order, and their respective onehot positions\n",
    "nom_pos = {\n",
    "    'Carácter del procedimiento' : [16,17]\n",
    "    'Forma del procedimiento' : [7,8,9]\n",
    "    'Entidad federativa' : [10,11,12,13,14,15]\n",
    "    'Tipo de contratación' : [21,22,23,24,25]\n",
    "    'Artículo' : [18,19,20]\n",
    "    'Plantilla' : [26,27,28,29,30,31]\n",
    "}\n",
    "# Dict that will contain filtered datasets\n",
    "filtered_data = {\n",
    "    'Carácter del procedimiento' : []\n",
    "    'Forma del procedimiento' : []\n",
    "    'Entidad federativa' : []\n",
    "    'Tipo de contratación' : []\n",
    "    'Artículo' : []\n",
    "    'Plantilla' : []\n",
    "}\n",
    "for key, value in nom_pos.items:\n",
    "    for onehot_att in value:\n",
    "        # Here modify pos vector then feed it into weka remove\n",
    "        # Change nom att to false\n",
    "        # Change only specific onehot to true\n",
    "        # See how to save it to change the class also\n",
    "        nominal_att_index = list(nom_pos.keys()).index(key)\n",
    "        pos_vector[nominal_att_index] = False\n",
    "        pos_vector[onehot_att] = True\n",
    "        remove = Filter(classname=\"weka.filters.unsupervised.attribute.Remove\", options=[\"-R\", pos_vector,\"-V\"])\n",
    "        remove.inputformat(data)\n",
    "        data.class_index()\n",
    "        filtered_data[key].append(remove.filter(data))\n",
    "            "
   ]
  },
  {
   "source": [
    "# Criterions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees: 1000\n",
    "trees = 150\n",
    "maxDepth = 10\n",
    "objectsByLeaf = 35\n",
    "\n",
    "input_config = f'weka.classifiers.trees.PBC4cip -S 1 -miner \\\"PRFramework.Core.SupervisedClassifiers.EmergingPatterns.Miners.RandomForestMinerWithoutFiltering -bagSizePercent 100 -numFeatures -1 -numTrees {trees} -builder \\\\\\\"PRFramework.Core.SupervisedClassifiers.DecisionTrees.Builder.DecisionTreeBuilder -distributionEvaluator \\\\\\\\\\\\\\\"PRFramework.Core.SupervisedClassifiers.DecisionTrees.DistributionEvaluators.QuinlanGain \\\\\\\\\\\\\\\" -maxDepth {maxDepth} \\\\\\\\\\\\\\\"-minimalObjByLeaf \\\\\\\\\\\\\\\" {objectsByLeaf} -minimalSplitGain 1.0E-30\\\\\\\"\\\"'\n",
    "\n",
    "classifier = from_commandline(input_config, classname=\"weka.classifiers.Classifier\")\n",
    "\n",
    "print(\">>> Building classifier...\")\n",
    "start_time_1 = time.time()\n",
    "classifier.build_classifier(data)\n",
    "print(f\">>> [Done] Bulding classifier. Time: {(time.time() - start_time_1)} seconds ---\")\n",
    "\n",
    "print(\">>> Serializing model...\")\n",
    "start_time_1 = time.time()\n",
    "classifier.serialize(f\"{filename}.model\")\n",
    "print(f\">>> [Done] Serializing model. Time: {(time.time() - start_time_1)} seconds ---\")\n",
    "\n",
    "print(\">>> Generating big string...\")\n",
    "start_time_1 = time.time()\n",
    "big_string = str(classifier).split(\"]\")\n",
    "print(f\">>> [Done] Generating big string. Time: {(time.time() - start_time_1)} seconds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_fields = []\n",
    "list_supports = []\n",
    "list_top100 = []\n",
    "list_top200 = []\n",
    "list_diff = []\n",
    "with tqdm(total=len(big_string)) as pbar:\n",
    "    for item in big_string:\n",
    "        text = \"\"\n",
    "        text = item + \"]\"\n",
    "        text = text.split(\"[\")\n",
    "        fields = text[0]\n",
    "        # print(fields)\n",
    "        if (len(text) > 1):\n",
    "            supports = text[1]\n",
    "            supports = supports[:-1]\n",
    "            suuports = supports.split()\n",
    "            supports_split = supports.split()\n",
    "            top100 = float(supports_split[0])\n",
    "            top200 = float(supports_split[1])\n",
    "            diff = float(top100 - top200)\n",
    "            list_fields.append(fields.strip())\n",
    "            list_supports.append(supports_split)\n",
    "            list_top100.append(top100)\n",
    "            list_top200.append(top200)\n",
    "            list_diff.append(diff)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['fields', 'supports', 'top100', 'top200', 'diff'])\n",
    "df['fields']=list_fields\n",
    "df['supports']=list_supports\n",
    "df['top100']=list_top100\n",
    "df['top200']=list_top200\n",
    "df['diff']=list_diff\n",
    "df.to_csv(f\"{filename}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=len(big_string), file=sys.stdout) as pbar:\n",
    "    with open(f'{filename}.txt', 'w') as f:\n",
    "        print(\"input: \" + input_config, file=f)\n",
    "        print(\"output: \" + classifier.to_commandline(), file=f)\n",
    "        print(\"model:\\n\", file=f)\n",
    "        for item in big_string:\n",
    "            text = item + \"]\"\n",
    "            print(text.strip(), file=f)\n",
    "            pbar.update(1)\n",
    "    \n",
    "print(f\"--- {(time.time() - start_time)} seconds ---\")\n",
    "    \n",
    "jvm.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}