{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.6 64-bit ('base': conda)",
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "f515a261c91f2d540a32fd338636eea76c71c483334d8e72b3eaaa34365c9198"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Pbc4cip- Python Implementation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import weka.core.jvm as jvm\n",
    "import weka.core.packages as packages\n",
    "from weka.core.classes import complete_classname\n",
    "from weka.core.converters import Loader\n",
    "from weka.classifiers import Classifier\n",
    "import weka.plot.graph as graph  # NB: pygraphviz and PIL are required\n",
    "from weka.core.classes import Random, from_commandline\n",
    "import weka.core.serialization as serialization\n",
    "from weka.filters import Filter\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Michael\n",
    "#wekafiles_path = \"C:/Users/mzenk/wekafiles\"\n",
    "#PBC4CIP_zip_path = \"C:/Users/mzenk/Google Drive/ITESM/Maestría/Semestre 3/ML2/Assignment4/PBC4cip-1.0-weka.zip\"\n",
    "#data_dir = \"C:/Users/mzenk/Google Drive/ITESM/Maestría/Semestre 3/ML2/Assignment4/Exped_Visualizations/\"\n",
    "#arff_file = \"weka_dataset_clean_noop_nocorreo.arff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dany\n",
    "wekafiles_path = \"/Users/dannygc/wekafiles\"\n",
    "PBC4CIP_zip_path = \"/Users/dannygc/Google Drive File Stream/My Drive/MCCNotes/MCC 3/Adv_ML/PBC_HW4/PBC4cip-1.0-weka.zip\"\n",
    "data_dir = \"/Volumes/GoogleDrive/My Drive/MCCNotes/Jlab projects/GITHUB_repositories/DRAE_repositories/Exped_Visualizations/\"\n",
    "arff_file = \"weka_dataset_clean_noop_nocorreo.arff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Constraints\n",
    "updated_PBC4CIP = False\n",
    "min_support_diff = 0.3\n",
    "min_count_diff = 30\n",
    "min_ratio = 0.6\n",
    "max_patterns = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "trees = 150\n",
    "maxDepth = 5\n",
    "objectsByLeaf = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "attname = 'Forma del procedimiento'\n",
    "attnum = 2\n",
    "filename = attname + str(attnum)\n",
    "\n",
    "trees = 15\n",
    "maxDepth = 10\n",
    "objectsByLeaf = 35\n",
    "\n",
    "# Dictionary with the nominal attributes in order, and their respective onehot indexes\n",
    "# Only change this if an attribute is moved or deleted\n",
    "nom_pos = {\n",
    "    'Caracter del procedimiento' : [18,19],\n",
    "    'Forma del procedimiento' : [7,8,9],\n",
    "    #'Operador' : [],\n",
    "    #'Correo electronico' : [],\n",
    "    'Entidad federativa' : [10,11,12,13,14,15],\n",
    "    'Tipo de contratación' : [21,22,23,24,25],\n",
    "    'Articulo' : [18,19,20],\n",
    "    'Plantilla' : [26,27,28,29,30,31]\n",
    "}\n",
    "class_index = len(nom_pos.keys()) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "6"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "len(nom_pos.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{&#39;Caracter del procedimiento&#39;: [18, 19],\n &#39;Forma del procedimiento&#39;: [7, 8, 9],\n &#39;Entidad federativa&#39;: [10, 11, 12, 13, 14, 15],\n &#39;Tipo de contratación&#39;: [21, 22, 23, 24, 25],\n &#39;Articulo&#39;: [18, 19, 20],\n &#39;Plantilla&#39;: [26, 27, 28, 29, 30, 31]}"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "nom_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = 'dany.pickle'\n",
    "load_pickle = True\n",
    "save_pickle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_pickle:\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        pickle.dump([   wekafiles_path,\n",
    "                        PBC4CIP_zip_path,\n",
    "                        data_dir,\n",
    "                        arff_file,\n",
    "                        updated_PBC4CIP], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_pickle:\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        wekafiles_path, PBC4CIP_zip_path, data_dir, arff_file, updated_PBC4CIP = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "DEBUG:weka.core.jvm:Adding bundled jars\nDEBUG:weka.core.jvm:Classpath=[&#39;/Users/dannygc/anaconda3/lib/python3.7/site-packages/javabridge/jars/rhino-1.7R4.jar&#39;, &#39;/Users/dannygc/anaconda3/lib/python3.7/site-packages/javabridge/jars/runnablequeue.jar&#39;, &#39;/Users/dannygc/anaconda3/lib/python3.7/site-packages/javabridge/jars/cpython.jar&#39;, &#39;/Users/dannygc/anaconda3/lib/python3.7/site-packages/weka/lib/python-weka-wrapper.jar&#39;, &#39;/Users/dannygc/anaconda3/lib/python3.7/site-packages/weka/lib/weka.jar&#39;]\nDEBUG:weka.core.jvm:MaxHeapSize=12g\nDEBUG:weka.core.jvm:Using alternative Weka home directory: /Users/dannygc/wekafiles\nDEBUG:weka.core.jvm:Using alternative Weka home directory: /Users/dannygc/wekafiles\n"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "#jvm.start(packages=True, max_heap_size=\"12g\") #max_heap_size 512m, 4g. packages=true searches for weka packages in installation program\n",
    "jvm.start(packages=wekafiles_path, max_heap_size='12g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "weka.classifiers.trees.PBC4cip\nPBC4cip is already installed.\n"
    }
   ],
   "source": [
    "pkg = \"PBC4cip\"\n",
    "print(complete_classname(\".\" + pkg))\n",
    "# install package if necessary \n",
    "if not packages.is_installed(pkg):\n",
    "    print(\"Installing %s...\" % pkg)\n",
    "    #packages.install_package(\"http://prdownloads.sourceforge.net/weka/discriminantAnalysis1.0.3.zip?download\")\n",
    "    packages.install_package(PBC4CIP_zip_path)\n",
    "    print(\"Installed %s, please re-run script!\" % pkg)\n",
    "    jvm.stop()\n",
    "    sys.exit(0)\n",
    "else:\n",
    "    print(pkg + \" is already installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n\n\n\n\n\n&gt;&gt;&gt; Start...\n"
    }
   ],
   "source": [
    "# testing classname completion\n",
    "\n",
    "print(\"\\n\\n\\n\\n\\n\")\n",
    "print(\">>> Start...\")\n",
    "\n",
    "loader = Loader(classname=\"weka.core.converters.ArffLoader\")\n",
    "data = loader.load_file(data_dir + arff_file)\n",
    "\n",
    "cmdline = []"
   ]
  },
  {
   "source": [
    "# Preprocess\n",
    "Here the one-hot attributes not used are deleted.\n",
    "Also the nominal attribute where the class being evaluated comes from is also deleted."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attributes = [f.name for f in data.attributes()]\n",
    "\n",
    "pos_vector = [False for i in range(len(attributes))]\n",
    "pos_vector[0:len(nom_pos.keys())] = [True for i in range(len(nom_pos.keys()))]\n",
    "pos_vector[len(attributes)-10:len(attributes)] = [True for i in range(10)]\n",
    "\n",
    "# Dict that will contain filtered datasets\n",
    "filtered_data = {\n",
    "    'Caracter del procedimiento' : [],\n",
    "    'Forma del procedimiento' : [],\n",
    "    'Entidad federativa' : [],\n",
    "    'Tipo de contratación' : [],\n",
    "    'Articulo' : [],\n",
    "    'Plantilla' : []\n",
    "}\n",
    "for key, value in nom_pos.items():\n",
    "    for onehot_att in value:\n",
    "        # Here modify pos vector then feed it into weka remove\n",
    "        # Change nom att to false\n",
    "        # Change only specific onehot to true\n",
    "        # See how to save it to change the class also\n",
    "        nominal_att_index = list(nom_pos.keys()).index(key)\n",
    "        pos_vector[nominal_att_index] = False\n",
    "        pos_vector[onehot_att-1] = True\n",
    "        indeces_not_filtered = [i+1 for i, val in enumerate(pos_vector) if val]\n",
    "        pos_vector[nominal_att_index] = True\n",
    "        pos_vector[onehot_att-1] = False\n",
    "        remove = Filter(classname=\"weka.filters.unsupervised.attribute.Remove\", options=[\"-R\",\",\".join(map(str, indeces_not_filtered)),\"-V\"])\n",
    "        remove.inputformat(data)\n",
    "        filtered_data[key].append(remove.filter(data))\n",
    "            "
   ]
  },
  {
   "source": [
    "# Criterions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Class attribute is: Forma_Procedimiento_Mixta\n&gt;&gt;&gt; Building classifier...\n&gt;&gt;&gt; [Done] Bulding classifier. Time: 6.743301868438721 seconds ---\n&gt;&gt;&gt; Serializing model...\n&gt;&gt;&gt; [Done] Serializing model. Time: 0.6984012126922607 seconds ---\n&gt;&gt;&gt; Generating big string...\n&gt;&gt;&gt; [Done] Generating big string. Time: 1.279534101486206 seconds ---\n"
    }
   ],
   "source": [
    "data = filtered_data[attname][attnum]\n",
    "data.class_index = class_index\n",
    "\n",
    "print(\"Class attribute is: \" + [f.name for f in data.attributes()][class_index])\n",
    "\n",
    "input_config = f'weka.classifiers.trees.PBC4cip -S 1 -miner \\\"PRFramework.Core.SupervisedClassifiers.EmergingPatterns.Miners.RandomForestMinerWithoutFiltering -bagSizePercent 100 -numFeatures -1 -numTrees {trees} -builder \\\\\\\"PRFramework.Core.SupervisedClassifiers.DecisionTrees.Builder.DecisionTreeBuilder -distributionEvaluator \\\\\\\\\\\\\\\"PRFramework.Core.SupervisedClassifiers.DecisionTrees.DistributionEvaluators.QuinlanGain \\\\\\\\\\\\\\\" -maxDepth {maxDepth} \\\\\\\\\\\\\\\"-minimalObjByLeaf \\\\\\\\\\\\\\\" {objectsByLeaf} -minimalSplitGain 1.0E-30\\\\\\\"\\\"'\n",
    "\n",
    "classifier = from_commandline(input_config, classname=\"weka.classifiers.Classifier\")\n",
    "\n",
    "print(\">>> Building classifier...\")\n",
    "start_time_1 = time.time()\n",
    "classifier.build_classifier(data)\n",
    "print(f\">>> [Done] Bulding classifier. Time: {(time.time() - start_time_1)} seconds ---\")\n",
    "\n",
    "print(\">>> Serializing model...\")\n",
    "start_time_1 = time.time()\n",
    "classifier.serialize(f\"{filename}.model\")\n",
    "print(f\">>> [Done] Serializing model. Time: {(time.time() - start_time_1)} seconds ---\")\n",
    "\n",
    "print(\">>> Generating big string...\")\n",
    "start_time_1 = time.time()\n",
    "big_string = str(classifier).split(\"]\\n\")\n",
    "print(f\">>> [Done] Generating big string. Time: {(time.time() - start_time_1)} seconds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "c0_count:  86888\nc1_count:  9234\nTotal:  96122\n"
    }
   ],
   "source": [
    "c0_count = 0\n",
    "c1_count = 0\n",
    "\n",
    "for instance in data:\n",
    "    if instance.values[class_index] == 0:\n",
    "        c0_count += 1\n",
    "    else:\n",
    "        c1_count += 1\n",
    "print('c0_count: ', c0_count)\n",
    "print('c1_count: ', c1_count)\n",
    "print('Total: ', c0_count + c1_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 2717/2717 [00:00&lt;00:00, 157626.51it/s]\n"
    }
   ],
   "source": [
    "list_fields = []\n",
    "list_support_c0 = []\n",
    "list_support_c1 = []\n",
    "list_num_c0 = []\n",
    "list_num_c1 = []\n",
    "list_diff = []\n",
    "with tqdm(total=len(big_string)) as pbar:\n",
    "    for item in big_string:\n",
    "        text = \"\"\n",
    "        text = item + \"]\"\n",
    "        text = text.split(\"[\")\n",
    "        fields = text[0]\n",
    "        # print(fields)\n",
    "        if updated_PBC4CIP:\n",
    "            if (len(text) > 1):\n",
    "                class_nums = text[1]\n",
    "                class_nums = class_nums[:-1]\n",
    "                class_nums = class_nums.split()\n",
    "                supports = text[2]\n",
    "                supports = supports[:-1]\n",
    "                supports = supports.split()\n",
    "                list_fields.append(fields.strip())\n",
    "                list_support_c0.append(float(supports[0]))\n",
    "                list_support_c1.append(float(supports[1]))\n",
    "                list_num_c0.append(float(class_nums[0]))\n",
    "                list_num_c1.append(float(class_nums[1]))\n",
    "        else:\n",
    "            if (len(text) > 1):\n",
    "                supports = text[1]\n",
    "                supports = supports[:-1]\n",
    "                supports = supports.split()\n",
    "                list_fields.append(fields.strip())\n",
    "                list_support_c0.append(float(supports[0]))\n",
    "                list_support_c1.append(float(supports[1]))\n",
    "                list_num_c0.append(float(supports[0])*c0_count)\n",
    "                list_num_c1.append(float(supports[1])*c1_count)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                 fields  support_c0  \\\n2711  Entidad federativa != &#39;bajio&#39; AND Articulo = &#39;...        0.09   \n2712  Articulo = &#39;41&#39; AND Most_used_UC_word_general ...        0.10   \n2713  Entidad federativa != &#39;bajio&#39; AND Most_used_UC...        0.10   \n2714  Tipo de contratacion = &#39;adquisiciones&#39; AND Ent...        0.11   \n2715  Caracter del procedimiento = &#39;nacional&#39; AND Mo...        0.12   \n\n      support_c1    num_c0  num_c1  \n2711        0.01   7819.92   92.34  \n2712        0.00   8688.80    0.00  \n2713        0.01   8688.80   92.34  \n2714        0.01   9557.68   92.34  \n2715        0.02  10426.56  184.68  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fields</th>\n      <th>support_c0</th>\n      <th>support_c1</th>\n      <th>num_c0</th>\n      <th>num_c1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2711</th>\n      <td>Entidad federativa != 'bajio' AND Articulo = '...</td>\n      <td>0.09</td>\n      <td>0.01</td>\n      <td>7819.92</td>\n      <td>92.34</td>\n    </tr>\n    <tr>\n      <th>2712</th>\n      <td>Articulo = '41' AND Most_used_UC_word_general ...</td>\n      <td>0.10</td>\n      <td>0.00</td>\n      <td>8688.80</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>2713</th>\n      <td>Entidad federativa != 'bajio' AND Most_used_UC...</td>\n      <td>0.10</td>\n      <td>0.01</td>\n      <td>8688.80</td>\n      <td>92.34</td>\n    </tr>\n    <tr>\n      <th>2714</th>\n      <td>Tipo de contratacion = 'adquisiciones' AND Ent...</td>\n      <td>0.11</td>\n      <td>0.01</td>\n      <td>9557.68</td>\n      <td>92.34</td>\n    </tr>\n    <tr>\n      <th>2715</th>\n      <td>Caracter del procedimiento = 'nacional' AND Mo...</td>\n      <td>0.12</td>\n      <td>0.02</td>\n      <td>10426.56</td>\n      <td>184.68</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "df = pd.DataFrame(columns = ['fields', 'support_c0', 'support_c1', 'num_c0', 'num_c1'])\n",
    "df['fields']=list_fields\n",
    "df['support_c0']=list_support_c0\n",
    "df['support_c1']= list_support_c1\n",
    "df['num_c0']= list_num_c0\n",
    "df['num_c1']= list_num_c1\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Empty DataFrame\nColumns: [fields, support_c0, support_c1, num_c0, num_c1, s_diff, n_diff, ratio]\nIndex: []",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fields</th>\n      <th>support_c0</th>\n      <th>support_c1</th>\n      <th>num_c0</th>\n      <th>num_c1</th>\n      <th>s_diff</th>\n      <th>n_diff</th>\n      <th>ratio</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "df['s_diff'] = df.support_c1 - df.support_c0\n",
    "df['n_diff'] = df.num_c1 - df.num_c0\n",
    "df['ratio'] = df.num_c1/(df.num_c0 + df.num_c1)\n",
    "\n",
    "df = df[df.s_diff > min_support_diff]\n",
    "df = df[df.n_diff > min_count_diff]\n",
    "df = df[df.ratio > min_ratio]\n",
    "\n",
    "df = df.sort_values(by=['s_diff'], ascending=False)\n",
    "df = df[:max_patterns]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "source": [
    "## Export to Patterns Obtained"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f\"Patterns_Obtained/{filename}.csv\", index=False)\n",
    "jvm.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}